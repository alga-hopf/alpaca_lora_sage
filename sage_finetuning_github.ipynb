{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alga-hopf/alpaca_lora_sage/blob/main/sage_finetuning_github.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8469e50-1d65-4aa8-b7ee-3f2154b7a96d",
      "metadata": {
        "id": "a8469e50-1d65-4aa8-b7ee-3f2154b7a96d"
      },
      "outputs": [],
      "source": [
        "!pip3 install transformers datasets Accelerate peft bitsandbytes sentencepiece wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import timeit\n",
        "import copy\n",
        "import torch\n",
        "from transformers import Trainer, TrainingArguments, LlamaForCausalLM, LlamaTokenizer, DataCollatorWithPadding, DataCollatorForSeq2Seq, get_scheduler, AdamW\n",
        "from datasets import load_dataset, DatasetDict, Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import wandb\n",
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "import random"
      ],
      "metadata": {
        "id": "Oz9nj8BGYSSQ"
      },
      "id": "Oz9nj8BGYSSQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "ORIhYMVk47MF"
      },
      "id": "ORIhYMVk47MF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a5d67333-0c9e-44f6-b0ce-bad4b5784f78",
      "metadata": {
        "id": "a5d67333-0c9e-44f6-b0ce-bad4b5784f78"
      },
      "source": [
        "# Load and inspect dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "o25NPbgiYitI"
      },
      "id": "o25NPbgiYitI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f51df9d-be9a-46e3-ba6c-7676be594c33",
      "metadata": {
        "id": "5f51df9d-be9a-46e3-ba6c-7676be594c33"
      },
      "outputs": [],
      "source": [
        "with open('your path to the dataset in google drive', 'r') as f:\n",
        "    raw_dataset = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5095501-48d0-4c69-ac17-fe19e31a9f60",
      "metadata": {
        "id": "d5095501-48d0-4c69-ac17-fe19e31a9f60"
      },
      "outputs": [],
      "source": [
        "len(raw_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "671849d8-1dd8-4538-8213-98d92b741f7d",
      "metadata": {
        "id": "671849d8-1dd8-4538-8213-98d92b741f7d"
      },
      "source": [
        "# Fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "id": "JP4Pwv4wwqd9"
      },
      "id": "JP4Pwv4wwqd9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90227a4b-df2e-4ca4-94fa-c8c4ee98aeb0",
      "metadata": {
        "id": "90227a4b-df2e-4ca4-94fa-c8c4ee98aeb0"
      },
      "outputs": [],
      "source": [
        "world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
        "model_max_length = 512\n",
        "batch_size = 128\n",
        "micro_batch_size = 4  \n",
        "lora_r = 8\n",
        "lora_alpha = 16\n",
        "lora_target_modules = [\"q_proj\", \"v_proj\"]\n",
        "lora_dropout = 0.05\n",
        "val_set_size = 2000\n",
        "ddp = world_size != 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bcbb8f1-9d48-4385-a1c3-847c41200066",
      "metadata": {
        "id": "3bcbb8f1-9d48-4385-a1c3-847c41200066"
      },
      "outputs": [],
      "source": [
        "tokenizer = LlamaTokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\", model_max_length=model_max_length, padding_side=\"right\", use_fast=False)\n",
        "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
        "DEFAULT_EOS_TOKEN = \"</s>\"\n",
        "DEFAULT_BOS_TOKEN = \"<s>\"\n",
        "DEFAULT_UNK_TOKEN = \"<unk>\"\n",
        "tokenizer.pad_token = DEFAULT_PAD_TOKEN\n",
        "tokenizer.eos_token = DEFAULT_EOS_TOKEN\n",
        "tokenizer.bos_token = DEFAULT_BOS_TOKEN\n",
        "tokenizer.unk_token = DEFAULT_UNK_TOKEN\n",
        "IGNORE_INDEX = -100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "564db8e3-8250-451a-b695-59e7b66562c9",
      "metadata": {
        "id": "564db8e3-8250-451a-b695-59e7b66562c9"
      },
      "outputs": [],
      "source": [
        "PROMPT_DICT = {\n",
        "    \"prompt_input\": (\n",
        "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
        "    ),\n",
        "    \"prompt_no_input\": (\n",
        "        \"Below is an instruction that describes a task. \"\n",
        "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
        "    ),\n",
        "}\n",
        "list_data_dict = raw_dataset\n",
        "prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]\n",
        "sources_list = [prompt_no_input.format_map(example) for example in list_data_dict]\n",
        "targets = [f\"{example['output']}{tokenizer.eos_token}\" for example in list_data_dict]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c65a77d-6715-4fc6-8d76-bd545afaba1b",
      "metadata": {
        "id": "1c65a77d-6715-4fc6-8d76-bd545afaba1b"
      },
      "outputs": [],
      "source": [
        "# Due to limited resources we train only on a subset of the dataset\n",
        "pop_size = 20000\n",
        "order = list(range(pop_size))\n",
        "random.shuffle(order)\n",
        "examples_list = [s + t for s, t in zip(sources_list, targets)]\n",
        "examples, sources = [], []\n",
        "for n in order:\n",
        "  examples.append(examples_list[n])\n",
        "  sources.append(sources_list[n])\n",
        "full_examples = {}\n",
        "all_examples, all_sources = [], []\n",
        "for i in range(len(examples)):\n",
        "    all_examples.append(examples[i])\n",
        "    all_sources.append(sources[i])\n",
        "full_examples[\"example\"] = all_examples\n",
        "full_examples[\"source\"] = all_sources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8c37c6d-6d2a-481d-919f-90aa89530d08",
      "metadata": {
        "id": "a8c37c6d-6d2a-481d-919f-90aa89530d08"
      },
      "outputs": [],
      "source": [
        "full_examples_dataset = Dataset.from_dict(full_examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "783d730d-77a4-4acc-9d5f-7ca6d20af675",
      "metadata": {
        "id": "783d730d-77a4-4acc-9d5f-7ca6d20af675"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(example):\n",
        "    data_dict = tokenizer(example[\"example\"], padding=\"longest\", max_length=model_max_length, truncation=True) \n",
        "    tokenized_source = tokenizer(example[\"source\"], padding=\"longest\", max_length=model_max_length, truncation=True) \n",
        "    data_dict[\"labels\"] = [IGNORE_INDEX] * len(tokenized_source[\"input_ids\"]) + data_dict[\"input_ids\"][len(tokenized_source[\"input_ids\"]):]\n",
        "    return data_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe4ef416-8750-43ea-929e-a156c2d43114",
      "metadata": {
        "id": "fe4ef416-8750-43ea-929e-a156c2d43114"
      },
      "outputs": [],
      "source": [
        "train_dataset = full_examples_dataset.map(tokenize_function)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.remove_columns([\"example\", \"source\"])"
      ],
      "metadata": {
        "id": "HtGl4DvecfQh"
      },
      "id": "HtGl4DvecfQh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "251dc89b-fd85-404e-81ec-c59e9d546523",
      "metadata": {
        "id": "251dc89b-fd85-404e-81ec-c59e9d546523"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4ff7ac8-5724-410e-aaef-87d67e3696cd",
      "metadata": {
        "id": "b4ff7ac8-5724-410e-aaef-87d67e3696cd"
      },
      "outputs": [],
      "source": [
        "gradient_accumulation_steps = batch_size // micro_batch_size\n",
        "device_map = \"auto\"\n",
        "world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
        "ddp = world_size != 1\n",
        "if ddp:\n",
        "    device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n",
        "    gradient_accumulation_steps = gradient_accumulation_steps // world_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dd7bb96-040c-4108-90f4-09758ff0fdc2",
      "metadata": {
        "id": "7dd7bb96-040c-4108-90f4-09758ff0fdc2"
      },
      "outputs": [],
      "source": [
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    get_peft_model_state_dict,\n",
        "    prepare_model_for_int8_training,\n",
        "    set_peft_model_state_dict,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07c31573-3769-4115-937a-f6735a26cecd",
      "metadata": {
        "id": "07c31573-3769-4115-937a-f6735a26cecd"
      },
      "outputs": [],
      "source": [
        "model = LlamaForCausalLM.from_pretrained(\"decapoda-research/llama-7b-hf\", load_in_8bit=True, torch_dtype=torch.float16, device_map=device_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "569a4a00-0e4c-4e1f-8643-fc144ce233b3",
      "metadata": {
        "id": "569a4a00-0e4c-4e1f-8643-fc144ce233b3"
      },
      "outputs": [],
      "source": [
        "model = prepare_model_for_int8_training(model)\n",
        "config = LoraConfig(\n",
        "    r=lora_r,\n",
        "    lora_alpha=lora_alpha,\n",
        "    target_modules=lora_target_modules,\n",
        "    lora_dropout=lora_dropout,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model = get_peft_model(model, config)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "warmup_steps = 100\n",
        "num_train_epochs = 3\n",
        "lr = 1e-4#3e-4\n",
        "optimizer = \"adamw_torch\"\n",
        "out_dir = \"/content/drive/MyDrive/\"\n",
        "batch_size = 16"
      ],
      "metadata": {
        "id": "MOQzi3fk8qsF"
      },
      "id": "MOQzi3fk8qsF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=data_collator)"
      ],
      "metadata": {
        "id": "BexQ3mSXOdtM"
      },
      "execution_count": null,
      "outputs": [],
      "id": "BexQ3mSXOdtM"
    },
    {
      "cell_type": "code",
      "source": [
        "project_name = \"sage_finetuning\"\n",
        "experiment_name = \"traning\"\n",
        "entity = \"your wandb entity here\""
      ],
      "metadata": {
        "id": "AhjIYgPCOdtN"
      },
      "execution_count": null,
      "outputs": [],
      "id": "AhjIYgPCOdtN"
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "id": "B2KJ2mxQOdtN"
      },
      "execution_count": null,
      "outputs": [],
      "id": "B2KJ2mxQOdtN"
    },
    {
      "cell_type": "code",
      "source": [
        "num_training_steps = num_train_epochs * len(train_dataloader)\n",
        "print('Num training steps:', num_training_steps)\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "opt = \"adamw\"\n",
        "\n",
        "if opt == \"adamw\":\n",
        "  optimizer = AdamW(model.parameters(), lr=lr)\n",
        "  config = {\n",
        "    \"lr\": lr,\n",
        "    \"optimizer\": opt,\n",
        "    \"epochs\": num_train_epochs,\n",
        "    \"batch_size\": batch_size\n",
        "  }\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=warmup_steps,\n",
        "    num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "save_checkpoint = 1000000000000\n",
        "wandb.init(project=project_name, name=experiment_name, config=config, entity=entity)\n",
        "\n",
        "t = 0\n",
        "for epoch in range(num_train_epochs):\n",
        "  model.train()\n",
        "  for batch in train_dataloader:\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    output = model(**batch)\n",
        "    loss = output.loss\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    def closure():\n",
        "      return loss\n",
        "    optimizer.step(closure)\n",
        "    if opt in [\"adam\", \"adamw\"]:\n",
        "      lr_scheduler.step()\n",
        "    progress_bar.update()\n",
        "    wandb.log({'loss': loss.item()})\n",
        "    t += 1\n",
        "    if t % save_checkpoint == 0:\n",
        "      model.save_pretrained(out_dir)\n",
        "    \n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "Mq_swvB4OdtP"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Mq_swvB4OdtP"
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(out_dir+\"alpaca_lora_sage\")"
      ],
      "metadata": {
        "id": "qFwjQc_MgrsI"
      },
      "id": "qFwjQc_MgrsI",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
